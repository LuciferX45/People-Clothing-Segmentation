{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuciferX45/People-Clothing-Segmentation/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXYeTh-gXNCF"
      },
      "source": [
        "# **People Clothing Segmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqw1kZOZXNCH"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N5j_WBsyXNCH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4_HxYXiXNCJ"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNGVmD4UXNCJ"
      },
      "outputs": [],
      "source": [
        "# Set up paths and constants\n",
        "DATASET_PATH =  kagglehub.dataset_download('rajkumarl/people-clothing-segmentation')\n",
        "IMAGE_DIR = os.path.join(DATASET_PATH, 'png_images/IMAGES')\n",
        "MASK_DIR = os.path.join(DATASET_PATH, 'png_masks/MASKS')\n",
        "CLASSES_FILE = os.path.join(DATASET_PATH, 'labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhwKUZppXNCJ"
      },
      "outputs": [],
      "source": [
        "IMAGE_HEIGHT = 544\n",
        "IMAGE_WIDTH = 824\n",
        "NUM_CLASSES = 60  # 59 clothing classes + background\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "VALIDATION_SPLIT = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AD8JqE9XNCK"
      },
      "source": [
        "## 2. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7iTvk3YXNCK"
      },
      "outputs": [],
      "source": [
        "def read_image(image_path):\n",
        "    \"\"\"Read and preprocess an image.\"\"\"\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBDzng09XNCK"
      },
      "outputs": [],
      "source": [
        "def resize_data(image, mask, height=IMAGE_HEIGHT, width=IMAGE_WIDTH):\n",
        "    \"\"\"Resize image and mask to the specified dimensions.\"\"\"\n",
        "    image = tf.image.resize(image, [height, width])\n",
        "    mask = tf.image.resize(mask, [height, width], method='nearest')\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nONHRjPLXNCK"
      },
      "outputs": [],
      "source": [
        "def augment_data(image, mask):\n",
        "    \"\"\"Apply data augmentation to image and mask.\"\"\"\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        mask = tf.image.flip_left_right(mask)\n",
        "\n",
        "    # Random brightness adjustment (only for image, not mask)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "\n",
        "    # Ensure image values stay in [0, 1]\n",
        "    image = tf.clip_by_value(image, 0, 1)\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Wfo7759XNCK"
      },
      "outputs": [],
      "source": [
        "def read_mask(mask_path):\n",
        "    \"\"\"Read and preprocess a segmentation mask.\"\"\"\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    # No need to normalize masks as they contain class indices\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo1UYcf8XNCK"
      },
      "outputs": [],
      "source": [
        "def prepare_sample(image_path, mask_path):\n",
        "    \"\"\"Prepare a single sample (image and mask).\"\"\"\n",
        "    image = read_image(image_path)\n",
        "    mask = read_mask(mask_path)\n",
        "\n",
        "    # Resize to target dimensions\n",
        "    image, mask = resize_data(image, mask)\n",
        "\n",
        "    # Apply augmentation during training\n",
        "    image, mask = augment_data(image, mask)\n",
        "\n",
        "    # Convert mask to one-hot encoding\n",
        "    mask = tf.squeeze(mask)  # Remove the channel dimension\n",
        "    mask = tf.one_hot(tf.cast(mask, tf.int32), NUM_CLASSES)\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyXsWjI-XNCK"
      },
      "outputs": [],
      "source": [
        "def create_dataset(image_paths, mask_paths):\n",
        "    \"\"\"Create a TensorFlow dataset from image and mask paths.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "    dataset = dataset.map(prepare_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM77KU_hXNCK"
      },
      "outputs": [],
      "source": [
        "# Create a function to read and process the image and mask\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load images and masks from the dataset and split into training/validation sets.\n",
        "    Returns: train_dataset, val_dataset, class_names\n",
        "    \"\"\"\n",
        "    # Read class names\n",
        "    class_df = pd.read_csv(CLASSES_FILE)\n",
        "    class_names = ['background'] + class_df['label_list'].tolist()\n",
        "\n",
        "    # Get all image filenames\n",
        "    image_files = sorted([os.path.join(IMAGE_DIR, fname) for fname in os.listdir(IMAGE_DIR)\n",
        "                          if fname.endswith('.png')])\n",
        "    mask_files = sorted([os.path.join(MASK_DIR, fname) for fname in os.listdir(MASK_DIR)\n",
        "                         if fname.endswith('.png')])\n",
        "\n",
        "    # Ensure we have matching number of images and masks\n",
        "    assert len(image_files) == len(mask_files), \"Number of images and masks don't match!\"\n",
        "    print(f\"Found {len(image_files)} images and masks\")\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_img_files, val_img_files, train_mask_files, val_mask_files = train_test_split(\n",
        "        image_files, mask_files, test_size=VALIDATION_SPLIT, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {len(train_img_files)} images\")\n",
        "    print(f\"Validation set: {len(val_img_files)} images\")\n",
        "\n",
        "    # Create TensorFlow datasets\n",
        "    train_dataset = create_dataset(train_img_files, train_mask_files)\n",
        "    val_dataset = create_dataset(val_img_files, val_mask_files)\n",
        "\n",
        "    return train_dataset, val_dataset, class_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJZuVrLtXNCL"
      },
      "source": [
        "## 3. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOUfnJvJXNCL"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    \"\"\"\n",
        "    Build a simpler U-Net model for semantic segmentation.\n",
        "    This implementation avoids dimension matching issues.\n",
        "    Returns: Compiled model\n",
        "    \"\"\"\n",
        "    # Define input layer\n",
        "    inputs = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
        "\n",
        "    # Contracting path (encoder)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    # Bridge\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    c4 = layers.Dropout(0.2)(c4)  # Add dropout for regularization\n",
        "\n",
        "    # Expansive path (decoder)\n",
        "    u5 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\n",
        "    u5 = layers.concatenate([u5, c3])\n",
        "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u5)\n",
        "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "    u6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = layers.concatenate([u6, c2])\n",
        "    c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "    u7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = layers.concatenate([u7, c1])\n",
        "    c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Conv2D(NUM_CLASSES, (1, 1), activation='softmax')(c7)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES)]\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgmvP5T0XNCL"
      },
      "source": [
        "## 4. Visualization Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiNEg6LIVuFU"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation loss and IoU.\"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mean_io_u'], label='Training IoU')\n",
        "    plt.plot(history.history['val_mean_io_u'], label='Validation IoU')\n",
        "    plt.title('IoU')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('IoU')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"training_history.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRDIzKkZVuFU"
      },
      "outputs": [],
      "source": [
        "def plot_evaluation_samples(model, val_data, num_samples=5):\n",
        "    \"\"\"Plot sample images, true masks, and predicted masks from validation data.\"\"\"\n",
        "    samples = val_data.take(num_samples)\n",
        "    plt.figure(figsize=(15, 25))\n",
        "    for i, (image, mask) in enumerate(samples):\n",
        "        pred_mask = model.predict(image)\n",
        "        for j in range(min(2, len(image))):\n",
        "            row = i * 2 + j\n",
        "            plt.subplot(num_samples * 2, 3, row * 3 + 1)\n",
        "            plt.imshow(image[j])\n",
        "            plt.title(f\"Image {row + 1}\")\n",
        "            plt.axis('off')\n",
        "            plt.subplot(num_samples * 2, 3, row * 3 + 2)\n",
        "            plt.imshow(tf.argmax(mask[j], axis=-1), cmap='nipy_spectral')\n",
        "            plt.title(f\"True Mask {row + 1}\")\n",
        "            plt.axis('off')\n",
        "            plt.subplot(num_samples * 2, 3, row * 3 + 3)\n",
        "            plt.imshow(tf.argmax(pred_mask[j], axis=-1), cmap='nipy_spectral')\n",
        "            plt.title(f\"Predicted Mask {row + 1}\")\n",
        "            plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"validation_results.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLops0nmXNCL"
      },
      "outputs": [],
      "source": [
        "# Custom callback to display segmentation results during training\n",
        "class SegmentationDisplayCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, dataset, num_samples=2):\n",
        "        self.dataset = dataset\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 2 == 0:  # Display every 2 epochs\n",
        "            samples = self.dataset.take(self.num_samples)\n",
        "            plt.figure(figsize=(15, 5 * self.num_samples))\n",
        "\n",
        "            for i, (image, mask) in enumerate(samples):\n",
        "                # Show original image\n",
        "                plt.subplot(self.num_samples, 3, i*3 + 1)\n",
        "                plt.imshow(image[0])\n",
        "                plt.title(f\"Image\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Show ground truth mask (only show the argmax class for visualization)\n",
        "                plt.subplot(self.num_samples, 3, i*3 + 2)\n",
        "                plt.imshow(tf.argmax(mask[0], axis=-1), cmap='nipy_spectral')\n",
        "                plt.title(f\"True Mask\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Show predicted mask\n",
        "                pred_mask = self.model.predict(image)\n",
        "                plt.subplot(self.num_samples, 3, i*3 + 3)\n",
        "                plt.imshow(tf.argmax(pred_mask[0], axis=-1), cmap='nipy_spectral')\n",
        "                plt.title(f\"Predicted Mask\")\n",
        "                plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"segmentation_epoch_{epoch}.png\")\n",
        "            plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-0ys3BVXNCL"
      },
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lEMiUpYXNCL"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(model, train_data, val_data):\n",
        "    \"\"\"\n",
        "    Train the model on the training set and validate on the validation set.\n",
        "    \"\"\"\n",
        "    # Set up callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            \"best_model.h5\", save_best_only=True, monitor=\"val_mean_io_u\"\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6\n",
        "        ),\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
        "        ),\n",
        "        SegmentationDisplayCallback(val_data)\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=val_data,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1x-qcAGXNCL"
      },
      "source": [
        "## 6. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLpMoyXPXNCL"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, val_data, class_names):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the validation set and display metrics.\n",
        "    \"\"\"\n",
        "    print(\"Evaluating model on validation set...\")\n",
        "    results = model.evaluate(val_data)\n",
        "\n",
        "    print(f\"Loss: {results[0]:.4f}\")\n",
        "    print(f\"Accuracy: {results[1]:.4f}\")\n",
        "    print(f\"Mean IoU: {results[2]:.4f}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RpUSOE7XNCL"
      },
      "source": [
        "## 7. Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yL-UOyv1XNCL",
        "outputId": "f24c7f8b-314e-47fc-afd5-097825ea751c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n",
            "Loading and preparing dataset...\n",
            "Found 1000 images and masks\n",
            "Training set: 800 images\n",
            "Validation set: 200 images\n",
            "Building model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 544, 824, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 544, 824, 64)         1792      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 544, 824, 64)         36928     ['conv2d[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 272, 412, 64)         0         ['conv2d_1[0][0]']            \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 272, 412, 128)        73856     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 272, 412, 128)        147584    ['conv2d_2[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 136, 206, 128)        0         ['conv2d_3[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 136, 206, 256)        295168    ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 136, 206, 256)        590080    ['conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 68, 103, 256)         0         ['conv2d_5[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 68, 103, 512)         1180160   ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 68, 103, 512)         2359808   ['conv2d_6[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 68, 103, 512)         0         ['conv2d_7[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, 136, 206, 256)        524544    ['dropout[0][0]']             \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 136, 206, 512)        0         ['conv2d_transpose[0][0]',    \n",
            "                                                                     'conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 136, 206, 256)        1179904   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 136, 206, 256)        590080    ['conv2d_8[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, 272, 412, 128)        131200    ['conv2d_9[0][0]']            \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 272, 412, 256)        0         ['conv2d_transpose_1[0][0]',  \n",
            " )                                                                   'conv2d_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 272, 412, 128)        295040    ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 272, 412, 128)        147584    ['conv2d_10[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 544, 824, 64)         32832     ['conv2d_11[0][0]']           \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 544, 824, 128)        0         ['conv2d_transpose_2[0][0]',  \n",
            " )                                                                   'conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 544, 824, 64)         73792     ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 544, 824, 64)         36928     ['conv2d_12[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 544, 824, 60)         3900      ['conv2d_13[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7701180 (29.38 MB)\n",
            "Trainable params: 7701180 (29.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Load data\n",
        "print(\"Loading and preparing dataset...\")\n",
        "train_data, val_data, class_names = load_data()\n",
        "\n",
        "# Build model\n",
        "print(\"Building model...\")\n",
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWQSyt4WVuFV",
        "outputId": "7dac8de8-6d4b-4291-a2fc-1bf035cca55d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "print(\"Training model...\")\n",
        "history = train_model(model, train_data, val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ihhkp33VuFV"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDd28zlRVuFV"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "print(\"Evaluating model...\")\n",
        "results = evaluate_model(model, val_data, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i9n0UpcVuFV"
      },
      "outputs": [],
      "source": [
        "plot_evaluation_samples(model, val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80ei2w_TVuFV"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('segmentation_model.h5')\n",
        "print(\"Model saved as 'segmentation_model.h5'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj99EF7JVuFV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Export model to TF SavedModel format\n",
        "tf.saved_model.save(model, 'saved_model')\n",
        "print(\"Model exported as SavedModel format in 'saved_model' directory\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pcs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}